# streamlit_app.py
import streamlit as st
import pandas as pd
import jieba
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter
import re
import base64
from io import BytesIO
import tempfile
import os

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ‹¼å¤šå¤šæ ‡é¢˜å…³é”®è¯åˆ†æ",
    page_icon="ğŸ“Š",
    layout="wide"
)

# è®¾ç½®ä¸­æ–‡åˆ†è¯
jieba.setLogLevel('WARN')

# è‡ªå®šä¹‰æ ·å¼
st.markdown("""
<style>
    .st-emotion-cache-1y4p8pa {
        padding: 2rem 1rem;
    }
    .title {
        color: #165DFF;
        text-align: center;
        margin-bottom: 2rem;
    }
    .upload-area {
        border: 2px dashed #165DFF;
        border-radius: 10px;
        padding: 2rem;
        text-align: center;
        margin-bottom: 2rem;
    }
    .wordcloud-container {
        display: flex;
        justify-content: center;
        margin-top: 2rem;
    }
</style>
""", unsafe_allow_html=True)

# è·å–åœç”¨è¯
def get_stopwords():
    try:
        with open("stopwords.txt", "r", encoding="utf-8") as f:
            return set([line.strip() for line in f])
    except FileNotFoundError:
        return {"çš„", "äº†", "å’Œ", "æ˜¯", "åœ¨", "æˆ‘", "æœ‰", "å°±", "ä¸", "äºº", "éƒ½", "ä¸€", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ",
                "åˆ°", "è¯´", "è¦", "å»", "ä½ ", "ä¼š", "ç€", "æ²¡æœ‰", "çœ‹", "å¥½", "è‡ªå·±", "è¿™"}

# é¢„å¤„ç†æ–‡æœ¬
def preprocess_text(titles, stopwords):
    all_words = []
    for title in titles:
        if not isinstance(title, str):
            title = str(title)
        title = re.sub(r'[^\u4e00-\u9fa5]', ' ', title)
        words = jieba.cut(title)
        filtered_words = [word for word in words if len(word) > 1 and word.strip() and word not in stopwords]
        all_words.extend(filtered_words)
    return all_words

# åˆ†æå…³é”®è¯
def analyze_keywords(words, top_n=100):
    word_counts = Counter(words)
    top_words = word_counts.most_common(top_n)
    return top_words, dict(word_counts)

# ç”Ÿæˆè¯äº‘
def generate_wordcloud(word_dict):
    wc = WordCloud(
        font_path="SimHei.ttf",  # ä½¿ç”¨å†…ç½®å­—ä½“æˆ–ä¸Šä¼ å­—ä½“æ–‡ä»¶
        width=800,
        height=600,
        background_color="white",
        max_words=200,
        contour_width=3,
        contour_color='steelblue'
    )
    wc.generate_from_frequencies(word_dict)
    return wc

# ä¸»åº”ç”¨
def main():
    st.markdown("<h1 class='title'>æ‹¼å¤šå¤šæ ‡é¢˜å…³é”®è¯åˆ†æ</h1>", unsafe_allow_html=True)
    
    # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
    with st.container():
        st.markdown("### ä¸Šä¼ å•†å“æ ‡é¢˜æ•°æ®")
        st.markdown("""
        <div class='upload-area'>
            <p>ä¸Šä¼ åŒ…å«æ‹¼å¤šå¤šå•†å“æ ‡é¢˜çš„Excelæ–‡ä»¶ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨åˆ†ææ ‡é¢˜ä¸­çš„é«˜é¢‘å…³é”®è¯å¹¶ç”Ÿæˆè¯äº‘å›¾</p>
        </div>
        """, unsafe_allow_html=True)
        
        uploaded_file = st.file_uploader(
            "é€‰æ‹©Excelæ–‡ä»¶ (.xlsx, .xls)", 
            type=["xlsx", "xls"],
            label_visibility="collapsed"
        )
    
    if uploaded_file is not None:
        try:
            # è¯»å–Excelæ–‡ä»¶
            df = pd.read_excel(uploaded_file)
            
            # æŸ¥æ‰¾æ ‡é¢˜åˆ—
            possible_columns = ['æ ‡é¢˜', 'å•†å“æ ‡é¢˜', 'product_title', 'title']
            title_column = None
            
            for col in possible_columns:
                if col in df.columns:
                    title_column = col
                    break
                    
            if title_column is None:
                st.error("æœªæ‰¾åˆ°æ ‡é¢˜åˆ—ï¼Œè¯·ç¡®ä¿ExcelåŒ…å«ä»¥ä¸‹åˆ—åä¹‹ä¸€: " + ", ".join(possible_columns))
                return
            
            titles = df[title_column].tolist()
            
            if not titles:
                st.error("Excelæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ ‡é¢˜æ•°æ®")
                return
            
            # æ˜¾ç¤ºè¿›åº¦
            with st.spinner("æ­£åœ¨åˆ†ææ•°æ®..."):
                # é¢„å¤„ç†å’Œåˆ†æ
                stopwords = get_stopwords()
                words = preprocess_text(titles, stopwords)
                top_words, word_dict = analyze_keywords(words)
                
                # æ˜¾ç¤ºç»“æœ
                st.success("åˆ†æå®Œæˆï¼")
                
                # åˆ›å»ºä¸¤åˆ—å¸ƒå±€
                col1, col2 = st.columns([1, 2])
                
                # å·¦ä¾§æ˜¾ç¤ºé«˜é¢‘è¯
                with col1:
                    st.markdown("### é«˜é¢‘å…³é”®è¯ (Top 20)")
                    for i, (word, count) in enumerate(top_words[:20], 1):
                        st.markdown(f"{i}. **{word}**: {count}æ¬¡")
                
                # å³ä¾§æ˜¾ç¤ºè¯äº‘
                with col2:
                    st.markdown("### å…³é”®è¯è¯äº‘å›¾")
                    wordcloud = generate_wordcloud(word_dict)
                    
                    # æ˜¾ç¤ºè¯äº‘
                    fig, ax = plt.subplots(figsize=(10, 8))
                    ax.imshow(wordcloud, interpolation='bilinear')
                    ax.axis('off')
                    st.pyplot(fig)
                    
                    # æ·»åŠ ä¸‹è½½æŒ‰é’®
                    buf = BytesIO()
                    plt.savefig(buf, format='png', bbox_inches='tight')
                    buf.seek(0)
                    st.download_button(
                        label="ä¸‹è½½è¯äº‘å›¾",
                        data=buf,
                        file_name="pdd_wordcloud.png",
                        mime="image/png"
                    )
        
        except Exception as e:
            st.error(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")

if __name__ == "__main__":
    main()